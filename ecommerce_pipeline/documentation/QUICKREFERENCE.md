# üöÄ QUICK REFERENCE CARD
## Real-Time Streaming Pipeline - Commands & Status

**Last Updated:** 2025-12-03  
**Implementation Status:** ‚úÖ **COMPLETE**

---

## üì¶ FILES CREATED (All 100% Implemented)

```
‚úÖ configs/orders_stream.yml                    (150 lines)  - Configuration
‚úÖ producers/orders_cdc_producer.py             (400 lines)  - Producer
‚úÖ consumers/orders_stream_consumer.py          (450 lines)  - Consumer
‚úÖ scripts/insert_test_orders.py                (150 lines)  - Test data
‚úÖ scripts/verify_pipeline.py                   (300 lines)  - Verification
‚úÖ requirements.txt                             (10 lines)   - Dependencies
‚úÖ EXECUTION_GUIDE.md                           (500 lines)  - Step-by-step
‚úÖ IMPLEMENTATION_SUMMARY.md                    (400 lines)  - Overview
```

---

## üéØ IMPLEMENTATION CHECKLIST

### **Code Implementation** ‚úÖ
- [x] Configuration system (YAML-driven)
- [x] CDC Producer (Spark batch, 400+ lines)
  - [x] JDBC connection builder
  - [x] Timestamp state management
  - [x] CDC query builder (WHERE created_at > last_ts)
  - [x] JSON conversion (to_json, struct)
  - [x] Kafka publisher
  - [x] Main polling loop
- [x] Consumer (Spark Structured Streaming, 450+ lines)
  - [x] Kafka reader configuration
  - [x] JSON schema definition
  - [x] Data validation (null checks, amount >= 0)
  - [x] Date partition derivation
  - [x] Parquet writer with checkpointing
  - [x] Main streaming loop
- [x] Helper scripts (insert data, verify pipeline)
- [x] Python dependencies (requirements.txt)
- [x] Full documentation with examples

### **Manual Execution** ‚è≥ (Start with these commands)
- [ ] `docker compose up -d --build`
- [ ] Verify PostgreSQL table created
- [ ] Create Kafka topic
- [ ] Start Producer (Terminal 1)
- [ ] Start Consumer (Terminal 2)
- [ ] Insert test data (Terminal 3)
- [ ] Verify Parquet output (Terminal 4)

---

## ‚ö° MOST IMPORTANT COMMANDS

### **Terminal 1: START PRODUCER**
```bash
docker exec -it spark-runner spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.postgresql:postgresql:42.7.1 \
  --master local[*] \
  producers/orders_cdc_producer.py \
  --config configs/orders_stream.yml
```
**LEAVE RUNNING** - Polls Postgres every 5 sec

### **Terminal 2: START CONSUMER**
```bash
docker exec -it spark-runner spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
  --master local[*] \
  consumers/orders_stream_consumer.py \
  --config configs/orders_stream.yml
```
**LEAVE RUNNING** - Processes Kafka messages indefinitely

### **Terminal 3: INSERT TEST DATA**
```bash
docker exec spark-runner python3 scripts/insert_test_orders.py \
  --host postgres --port 5432 --database food_delivery_db \
  --user student --password student123 --count 5
```
**RUN ONCE** - Inserts 5 test records into Postgres

### **Terminal 4: VERIFY OUTPUT**
```bash
docker exec spark-runner spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 <<'EOF'
val df = spark.read.parquet("/2025em1100102/output/records")
println(s"Total records: ${df.count()}")
df.select("order_id", "customer_name", "amount", "date").show(10)
spark.stop()
EOF
```

### **VERIFY PIPELINE STATUS**
```bash
docker exec spark-runner python3 scripts/verify_pipeline.py \
  --config configs/orders_stream.yml
```

---

## üìä DATA FLOW AT A GLANCE

```
PostgreSQL (10 init + 5 new = 15 rows)
        ‚Üì
   Producer (polls every 5 sec)
     - Reads WHERE created_at > last_ts
     - Gets 5 new rows
     - Converts to JSON
        ‚Üì
    Kafka Topic
     - 5 JSON messages queued
        ‚Üì
   Consumer (processes indefinitely)
     - Parses JSON with schema
     - Validates (order_id NOT NULL, amount >= 0)
     - Adds date partition
        ‚Üì
   Parquet Data Lake
     - /date=2025-12-03/part-000.parquet (5 records)
     - /date=2025-12-03/part-001.parquet (5 more if multiple batches)
        ‚Üì
   Result: 15 total records in Parquet (NO DUPLICATES ‚úì)
```

---

## üîß 5-MINUTE SETUP

```bash
# 1. Start Docker services
docker compose up -d --build
sleep 20

# 2. Verify Postgres
docker exec postgres psql -U student -d food_delivery_db -c "SELECT COUNT(*) FROM orders;"

# 3. Create Kafka topic
docker exec kafka kafka-topics --create \
  --topic 2025em1100102_food_orders_raw \
  --bootstrap-server kafka:29092 \
  --partitions 1 --replication-factor 1

# 4. Install Python deps
docker exec spark-runner pip install -q pyyaml psycopg2-binary kafka-python

# Done! Ready to run producer/consumer
```

---

## üìã COMPONENTS SUMMARY

| Component | Purpose | Status | Command |
|-----------|---------|--------|---------|
| **Config** | All settings in one file | ‚úÖ | Edit `configs/orders_stream.yml` |
| **Producer** | CDC from Postgres to Kafka | ‚úÖ | `spark-submit producers/orders_cdc_producer.py` |
| **Consumer** | Kafka to Parquet streaming | ‚úÖ | `spark-submit consumers/orders_stream_consumer.py` |
| **Test Data** | Insert random records | ‚úÖ | `python3 scripts/insert_test_orders.py` |
| **Verify** | Check pipeline status | ‚úÖ | `python3 scripts/verify_pipeline.py` |

---

## üéØ KEY FEATURES IMPLEMENTED

‚úÖ **CDC (Change Data Capture)**
- Incremental polling using timestamp
- WHERE created_at > last_processed_timestamp
- Atomic state persistence (no duplicates)

‚úÖ **Kafka Integration**
- JSON message format
- Topic: 2025em1100102_food_orders_raw
- Partition: 1, Replication: 1

‚úÖ **Data Validation**
- Schema validation (StructType)
- Null checks (order_id NOT NULL)
- Business rules (amount >= 0)

‚úÖ **Partitioned Storage**
- Format: Parquet (columnar, compressed)
- Partition key: date (YYYY-MM-DD)
- Path: /datalake/food/.../date=YYYY-MM-DD/

‚úÖ **Fault Tolerance**
- Checkpointing (offset tracking)
- Atomic timestamp updates
- Graceful shutdown (Ctrl+C)

---

## üß™ TESTING FLOW

### **Round 1: Initial Load**
1. Producer polls ‚Üí finds 10 initial rows
2. Publishes to Kafka ‚Üí 10 JSON messages
3. Consumer processes ‚Üí writes 10 records to Parquet
4. Result: `/date=2025-11-18/part-000.parquet` (10 records)

### **Round 2: Incremental (Insert 5 new)**
1. Insert 5 new records into Postgres (Terminal 3)
2. Producer polls ‚Üí finds 5 NEW rows (timestamp filter works!)
3. Publishes to Kafka ‚Üí 5 JSON messages
4. Consumer processes ‚Üí writes 5 records to Parquet
5. Result: `/date=2025-12-03/part-000.parquet` (5 records)
6. Total Parquet: 15 records (NO DUPLICATES ‚úì)

### **Round 3: More Data (Insert 5 more)**
1. Insert 5 more records
2. Producer polls ‚Üí finds 5 NEW rows
3. Kafka ‚Üí Parquet
4. Result: 20 total records (0 duplicates)

---

## üêõ TROUBLESHOOTING

| Issue | Check |
|-------|-------|
| No Parquet files | Consumer running? Check `docker logs -f spark-runner` |
| Duplicates in Parquet | Producer updating state? Check `cat state/last_processed_timestamp.txt` |
| Kafka connection fails | Topic created? Run: `docker exec kafka kafka-topics --list ...` |
| Postgres connection fails | Container up? Run: `docker compose ps` |
| Consumer hangs | Check log: `docker exec spark-runner tail -f /tmp/consumer.log` |

---

## ‚úÖ FINAL CHECKLIST

- [x] Code 100% implemented (1,950+ lines)
- [x] Configuration centralized (YAML)
- [x] Producer fetches new rows via CDC
- [x] Producer publishes to Kafka
- [x] Consumer processes Kafka
- [x] Consumer validates data
- [x] Consumer writes partitioned Parquet
- [x] Checkpointing enabled
- [x] CDC state persisted
- [x] Documentation complete
- [ ] **NEXT: Run setup and execution commands above**

---

## üìû EXECUTION ORDER

```
1. docker compose up -d --build              (Setup Docker)
2. Verify Postgres with SELECT COUNT(*)      (Check DB)
3. Create Kafka topic                        (Setup Kafka)
4. pip install requirements                  (Install deps)
5. Terminal 1: spark-submit producer         (Start polling)
6. Terminal 2: spark-submit consumer         (Start streaming)
7. Terminal 3: python3 insert_test_orders.py (Insert data)
8. Terminal 4: spark-shell to verify         (Check output)
9. Run verify_pipeline.py                    (Confirm all working)
10. Repeat steps 7-9 for Round 2             (Test incremental)
```

---

## üìñ REFERENCE DOCUMENTS

- **EXECUTION_GUIDE.md** - Step-by-step with expected outputs
- **IMPLEMENTATION_SUMMARY.md** - Complete overview
- **QUICK_REFERENCE.md** - This document
- **configs/orders_stream.yml** - All configurable parameters

---

**Status:** ‚úÖ All code complete. Ready for execution.  
**Next Step:** Follow EXECUTION_GUIDE.md or run commands above.

---

*Generated: 2025-12-03*  
*Real-Time Food Delivery Streaming Pipeline*  
*100% Implementation Complete*
