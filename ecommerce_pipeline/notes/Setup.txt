Provided is a docker setup project, basis which below are to be performed.
Generate step by step path to implement the desired outcome in an incremental manner.

# âœ… **1. Key Insights From the Assignment**

Based on the full review of *Assignment #2: Real-Time Food Delivery Streaming Pipeline* :

### **1.1 You Are Building a True Real-Time Streaming Pipeline**

The architecture integrates:

* **PostgreSQL** â†’ source OLTP database
* **Kafka** â†’ real-time event transport
* **Spark Structured Streaming** â†’ ingestion + cleaning + transformation
* **Data Lake (Parquet)** â†’ final analytical storage

This mimics real-world food-delivery systems (Zomato/Swiggy) where operational events flow into analytical systems.

---

### **1.2 Incremental CDC (Change Data Capture) Is the Core Logic**

You must detect only new rows using:

```
created_at > last_processed_timestamp
```

This is manually persisted outside the DB (local file), ensuring:

* No duplicates
* Correct incremental streaming
* Simple, deterministic CDC pattern

(Modern CDC tools like Debezium/Kafka Connect automate this, but the assignment wants you to implement CDC manually.)

---

### **1.3 Producer Is a Spark Application â€” Not Plain Python**

Unique twist:
Your **PostgreSQL â†’ Kafka producer is a Spark Structured Streaming program**, not a standalone Python script.

This is unusual but intentional:

* Spark handles polling
* Spark offers schema consistency
* You already import Kafka connector JARs

---

### **1.4 The Consumer Is a Continuous Spark Streaming Job**

It must:

1. Read Kafka JSON
2. Parse using defined schema
3. Apply data cleaning:

   * No null order_id
   * No negative amount
4. Write to **partitioned Parquet** in s3 bucket, partitioned by:

   ```
   date = YYYY-MM-DD  (extracted from created_at)
   ```
5. Maintain offsets using checkpointing

This is a **stateful** streaming job, not batch.

---

### **1.5 End-to-End Evaluation Mimics Production Monitoring**

Instructor will:

* Insert 5 records
* Run pipeline
* Validate correctness in data lake
* Repeat multiple increments

This checks:

* Your CDC works
* No duplicates
* Checkpointing is correct
* Partitioning is correct

---

### **1.6 Config-Driven System**

All paths and variables must come from:

```
configs/orders_stream.yml
```

This ensures:

* Portability
* Single configuration source
* No hardcoding

---

## **Step 1 â€” Create PostgreSQL Table & Insert Initial Data**

Run `orders.sql`:

* Creates orders table
* Inserts 10 base records

Ensure timestamps are correct.

---

## **Step 2 â€” Start Kafka Infrastructure**

```
brew services start zookeeper
brew services start kafka
```

Create topic:

```
kafka-topics --create --topic <rollnumber>_food_orders_raw --bootstrap-server localhost:9092
```

---

## **Step 3 â€” Implement & Test Producer (CDC Poller)**

Script: `orders_cdc_producer.py`.

Runs every 5 sec:

1. Read last_processed_timestamp from file
2. Query PostgreSQL for new rows
3. Convert to JSON
4. Publish to Kafka

### Run producer:

```
source de_env/bin/activate
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    producers/orders_cdc_producer.py \
    --config configs/orders_stream.yml
```

docker exec -it spark-runner /opt/spark/bin/spark-submit \
  --packages "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,org.postgresql:postgresql:42.7.1" \
  --master "local[*]" \
  --driver-memory 2g \
  --executor-memory 2g \
  producers/orders_cdc_producer.py \
  --config configs/orders_stream.yml

---

## **Step 4 â€” Implement & Run the Spark Consumer**

Script: `orders_stream_consumer.py`.

It must:

* Read Kafka events
* Parse JSON schema
* Validate / clean
* Derive date partition
* Write parquet partitioned
* Use checkpointing

### Run consumer:

```
source de_env/bin/activate
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    consumers/orders_stream_consumer.py \
    --config configs/orders_stream.yml
```

Now Spark listens forever.

---

## **Step 5 â€” Testing the Full Pipeline**

### **Round 1**

1. Insert 5 new rows into PostgreSQL:

   ```
   INSERT INTO <rollnumber>_orders(...) VALUES (...);
   ```
2. Watch producer publish JSON â†’ Kafka
3. Watch consumer process â†’ Parquet

Verify:

```
/datalake/food/<rollnumber>/output/orders/date=YYYY-MM-DD/*.parquet
```

### **Round 2**

Insert 5 more rows.

Validate:

* Counts increased correctly
* No duplicates
* Timestamp moved forward
* Checkpoint folder updated

Repeat until confident.

---

# ðŸŽ¯ **4. Final Summary: How the Entire Pipeline Works**

### â–¶ **Data Creation**

User places food order â†’ new row inserted into **PostgreSQL**.

### â–¶ **CDC Producer (Spark)**

* Polls PostgreSQL for new rows
* Converts them to JSON
* Sends them to **Kafka topic**

### â–¶ **Kafka Broker**

* Stores incoming messages
* Makes them available for real-time consumption

### â–¶ **Spark Structured Streaming Consumer**

* Reads Kafka JSON
* Cleans & transforms data
* Extracts partition date
* Writes append-only **Parquet** files
* Uses **checkpointing** for offset tracking

### â–¶ **Data Lake**

Partitioned by:

```
date=YYYY-MM-DD/
```

### â–¶ **Analytics / Downstream**

Data now ready for:

* BI dashboards
* ML models
* Warehouse loading
* Feature engineering

---

# If you want, I can also generate:

âœ… Producer code template
âœ… Consumer code template
âœ… README.md
âœ… Fully working SQL scripts
âœ… Test data generators
âœ… Spark submit helper scripts

Just tell me â€” I can build the entire submission-ready project structure.

---

docker compose up -build
docker compose up -d -build

Kafka: xeotek/kadeck, webui.
  https://help.xeotek.com/portal/en/kb/articles/desktop-download-and-startup#Windows
  https://www.datastreamhouse.com/get-started

Connection details: 127.0.0.1:9092.

https://www.youtube.com/watch?v=zJ6WbK9zFpI

---

81194246@HQN473V4C2 2025em1100102 % docker compose down
[+] Running 8/8
 âœ” Container spark-runner         Removed                                                                                                                                0.0s 
 âœ” Container spark-worker-a       Removed                                                                                                                                0.7s 
 âœ” Container spark-worker-b       Removed                                                                                                                                0.0s 
 âœ” Container kafka                Removed                                                                                                                                0.0s 
 âœ” Container postgres             Removed                                                                                                                                0.3s 
 âœ” Container zookeeper            Removed                                                                                                                                0.7s 
 âœ” Container spark-master         Removed                                                                                                                                0.6s 
 âœ” Network 2025em1100102_default  Removed    