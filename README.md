# Real-Time Food Delivery Streaming Pipeline

## üìã Project Overview

This is a **real-time data streaming pipeline** for a food delivery system that captures order events from a PostgreSQL database, streams them through Apache Kafka, and persists them to a data lake using Apache Spark. The system implements **Change Data Capture (CDC)** patterns to incrementally capture new orders and process them in real-time.

### Key Features
- **Real-time CDC**: Incremental polling of PostgreSQL for new orders
- **Kafka Streaming**: Publish-subscribe messaging for order events
- **Spark Structured Streaming**: Continuous processing and validation
- **Data Lake Storage**: Partitioned Parquet files organized by date
- **Docker Containerization**: Complete isolated environment with all services
- **Comprehensive Validation**: Data quality checks and error handling

---

## üèóÔ∏è Architecture Overview

### System Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    REAL-TIME PIPELINE FLOW                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PostgreSQL Database          Kafka Message Broker         Data Lake
(Source)                     (Message Queue)              (Sink)
    ‚îÇ                             ‚îÇ                          ‚îÇ
    ‚îÇ  1. CDC Polling             ‚îÇ                          ‚îÇ
    ‚îÇ  (every 5 sec)              ‚îÇ                          ‚îÇ
    ‚îÇ                             ‚îÇ                          ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                          ‚îÇ
    ‚îÇ orders_cdc_producer.py      ‚îÇ                          ‚îÇ
    ‚îÇ - Query new orders          ‚îÇ                          ‚îÇ
    ‚îÇ - Convert to JSON           ‚îÇ                          ‚îÇ
    ‚îÇ - Publish to Kafka          ‚îÇ                          ‚îÇ
    ‚îÇ                             ‚îÇ                          ‚îÇ
    ‚îÇ                    2. Stream Processing                ‚îÇ
    ‚îÇ                             ‚îÇ                          ‚îÇ
    ‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
    ‚îÇ                    ‚îÇ Kafka Topic:    ‚îÇ                 ‚îÇ
    ‚îÇ                    ‚îÇ 2025em1100102_  ‚îÇ                 ‚îÇ
    ‚îÇ                    ‚îÇ food_orders_raw ‚îÇ                 ‚îÇ
    ‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
    ‚îÇ                             ‚îÇ                          ‚îÇ
    ‚îÇ                             ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ                             ‚îÇ orders_stream_consumer.py‚îÇ
    ‚îÇ                             ‚îÇ - Read from Kafka       ‚îÇ
    ‚îÇ                             ‚îÇ - Parse JSON            ‚îÇ
    ‚îÇ                             ‚îÇ - Validate data         ‚îÇ
    ‚îÇ                             ‚îÇ - Add date partition    ‚îÇ
    ‚îÇ                             ‚îÇ - Write Parquet         ‚îÇ
    ‚îÇ                             ‚îÇ                          ‚îÇ
    ‚îÇ                             ‚îÇ      3. Persist         ‚îÇ
    ‚îÇ                             ‚îÇ                          ‚îÇ
    ‚îÇ                             ‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ                             ‚îÇ      ‚îÇ /datalake/food/  ‚îÇ
    ‚îÇ                             ‚îÇ      ‚îÇ 2025em1100102/   ‚îÇ
    ‚îÇ                             ‚îÇ      ‚îÇ output/orders/   ‚îÇ
    ‚îÇ                             ‚îÇ      ‚îÇ date=YYYY-MM-DD/ ‚îÇ
    ‚îÇ                             ‚îÇ      ‚îÇ part-*.parquet   ‚îÇ
    ‚îÇ                             ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technology Stack

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Message Queue** | Apache Kafka | 7.5.0 | Real-time event streaming |
| **Coordination** | Apache Zookeeper | 7.5.0 | Kafka cluster management |
| **Stream Processing** | Apache Spark | 3.4.0 | Distributed data processing |
| **Database** | PostgreSQL | 13 | Source of truth for orders |
| **Storage Format** | Parquet | - | Columnar storage in data lake |
| **Configuration** | YAML | - | Centralized pipeline config |
| **Containerization** | Docker Compose | - | Service orchestration |

---

## üìÅ Project Structure

```
2025em1100102/
‚îú‚îÄ‚îÄ Dockerfile                          # Custom Spark image with dependencies
‚îú‚îÄ‚îÄ docker-compose.yml                  # Service definitions (Spark, Kafka, Postgres)
‚îú‚îÄ‚îÄ spark-defaults.conf                 # Spark configuration
‚îÇ
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ orders_stream.yml              # Central configuration file (YAML)
‚îÇ
‚îú‚îÄ‚îÄ producers/
‚îÇ   ‚îî‚îÄ‚îÄ orders_cdc_producer.py          # CDC polling + Kafka publishing
‚îÇ
‚îú‚îÄ‚îÄ consumers/
‚îÇ   ‚îî‚îÄ‚îÄ orders_stream_consumer.py       # Kafka reading + Spark streaming + Parquet writing
‚îÇ
‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îî‚îÄ‚îÄ orders.sql                      # PostgreSQL schema + sample data
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ producer_spark_submit.sh        # Launch producer in Spark cluster
‚îÇ   ‚îú‚îÄ‚îÄ consumer_spark_submit.sh        # Launch consumer in Spark cluster
‚îÇ   ‚îú‚îÄ‚îÄ insert_test_orders.py           # Insert test data into PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ test_postgres_connection.py     # Verify PostgreSQL connectivity
‚îÇ   ‚îú‚îÄ‚îÄ test_kafka_producer.py          # Test Kafka producer
‚îÇ   ‚îú‚îÄ‚îÄ test_kafka_consumer.py          # Test Kafka consumer
‚îÇ   ‚îú‚îÄ‚îÄ verify_pipeline.py              # End-to-end pipeline verification
‚îÇ   ‚îú‚îÄ‚îÄ read_parquet_records.py         # Read and display Parquet files
‚îÇ   ‚îî‚îÄ‚îÄ create_parquet_*.py             # Utility scripts for Parquet operations
‚îÇ
‚îú‚îÄ‚îÄ setup/
‚îÇ   ‚îî‚îÄ‚îÄ docker/                         # Docker setup utilities
‚îÇ
‚îú‚îÄ‚îÄ logs/                               # Application logs
‚îú‚îÄ‚îÄ postgres_data/                      # PostgreSQL persistent data volume
‚îî‚îÄ‚îÄ datalake/                           # Data lake output directory (created at runtime)
```

---

## üöÄ Quick Start Guide

### Prerequisites

- **Docker & Docker Compose** installed
- **Python 3.8+** (for local script execution)
- **4GB+ RAM** available for containers
- **Disk space**: ~2GB for data volumes

### Step 1: Clone/Navigate to Project

```bash
cd /path/to/2025em1100102
```

### Step 2: Start All Services

```bash
docker-compose up -d
```

This starts:
- **Spark Master** (port 9090)
- **Spark Workers** (ports 9091, 9785)
- **PostgreSQL** (port 5432)
- **Kafka** (port 29095)
- **Zookeeper** (port 2888)

### Step 3: Verify Services are Running

```bash
docker-compose ps
```

Expected output:
```
NAME                COMMAND                  SERVICE             STATUS
spark-master        /opt/spark/bin/spark-... spark-master        Up
spark-worker-a      /opt/spark/bin/spark-... spark-worker-a      Up
spark-worker-b      /opt/spark/bin/spark-... spark-worker-b      Up
postgres            docker-entrypoint.s...   postgres            Up
kafka               /etc/confluent/docker... kafka               Up
zookeeper           /etc/confluent/docker... zookeeper           Up
spark-runner        tail -f /dev/null        spark-runner        Up
```

### Step 4: Insert Test Data (Optional)

```bash
docker-compose exec spark-runner python3 scripts/insert_test_orders.py --config configs/orders_stream.yml
```

### Step 5: Start Producer (CDC Polling)

```bash
docker-compose exec spark-runner bash scripts/producer_spark_submit.sh
```

This starts polling PostgreSQL every 5 seconds for new orders.

### Step 6: Start Consumer (Streaming Processing)

In a new terminal:
```bash
docker-compose exec spark-runner bash scripts/consumer_spark_submit.sh
```

This reads from Kafka and writes to Parquet files.

### Step 7: Monitor Pipeline

```bash
docker-compose exec spark-runner python3 scripts/verify_pipeline.py --config configs/orders_stream.yml
```

---

## üîß Configuration Guide

### Central Configuration File: `configs/orders_stream.yml`

All pipeline parameters are centralized in this YAML file. Key sections:

#### PostgreSQL Configuration
```yaml
postgres:
  host: postgres                    # Docker service name
  port: 5432
  db: food_delivery_db
  user: student
  password: student123
  driver: org.postgresql.Driver
  table: 2025em1100102_orders       # Source table
```

#### Kafka Configuration
```yaml
kafka:
  brokers: kafka:9095               # Docker service name
  topic: 2025em1100102_food_orders_raw
  num_partitions: 3
  replication_factor: 1
  consumer_group: food_orders_consumer_group
```

#### Streaming Configuration
```yaml
streaming:
  checkpoint_location: /app/datalake/food/2025em1100102/checkpoints/orders
  last_processed_timestamp_location: /app/datalake/food/2025em1100102/lastprocess/orders
  batch_interval: 5                 # seconds

cdc:
  poll_interval_sec: 5              # How often to query PostgreSQL
  batch_limit: 1000                 # Max records per poll
  default_start_timestamp: "2025-11-18 00:00:00"
```

#### Data Lake Configuration
```yaml
datalake:
  path: /app/datalake/food/2025em1100102/output/orders
  format: parquet
```

#### Consumer Configuration
```yaml
consumer:
  maxOffsetsPerTrigger: 1000        # Batch size control
  trigger_interval_ms: 5000         # Process every 5 seconds
  await_termination_timeout: 0      # Run indefinitely
```

#### Data Validation Rules
```yaml
validation:
  allow_null_order_id: false
  allow_negative_amount: false
  drop_invalid_rows: true
```

---

## üìä Data Flow & Workflow

### End-to-End Processing Flow

#### 1. **CDC Producer Phase** (`orders_cdc_producer.py`)

**Purpose**: Incrementally capture new orders from PostgreSQL

**Process**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ POLL LOOP (every 5 seconds)             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Read last_processed_timestamp        ‚îÇ
‚îÇ    from state file                      ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ 2. Query PostgreSQL:                    ‚îÇ
‚îÇ    SELECT * FROM orders                 ‚îÇ
‚îÇ    WHERE created_at > last_timestamp    ‚îÇ
‚îÇ    ORDER BY created_at ASC              ‚îÇ
‚îÇ    LIMIT 1000                           ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ 3. Convert rows to JSON:                ‚îÇ
‚îÇ    {"order_id": 1, "customer_name": ..} ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ 4. Publish to Kafka topic               ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ 5. Update state file with:              ‚îÇ
‚îÇ    max(created_at)|max(order_id)        ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ 6. Sleep 5 seconds                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Features**:
- **Incremental Processing**: Only fetches records with `created_at > last_timestamp`
- **Tie-breaker Logic**: Uses `order_id` to handle records with same timestamp
- **Atomic State Management**: Updates state only after successful Kafka publish
- **Graceful Shutdown**: Handles Ctrl+C cleanly

**State File Format**:
```
2025-11-18 12:30:45.123456|42
‚îú‚îÄ Timestamp with microseconds
‚îî‚îÄ Last order_id at that timestamp
```

#### 2. **Kafka Message Queue**

**Topic**: `2025em1100102_food_orders_raw`
- **Partitions**: 3 (for parallelism)
- **Replication Factor**: 1 (single broker)
- **Message Format**: JSON strings

**Example Message**:
```json
{
  "order_id": 1,
  "customer_name": "Alice Smith",
  "restaurant_name": "Spice Garden",
  "item": "Butter Chicken",
  "amount": 350.00,
  "order_status": "DELIVERED",
  "created_at": "2025-11-18T10:00:00Z"
}
```

#### 3. **Spark Structured Streaming Consumer** (`orders_stream_consumer.py`)

**Purpose**: Validate, enrich, and persist streaming data

**Process**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STREAMING QUERY (continuous)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Read from Kafka topic             ‚îÇ
‚îÇ    (startingOffsets: earliest)        ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ 2. Parse JSON messages               ‚îÇ
‚îÇ    using predefined schema            ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ 3. Apply validation rules:           ‚îÇ
‚îÇ    ‚úì order_id NOT NULL               ‚îÇ
‚îÇ    ‚úì amount > 0                      ‚îÇ
‚îÇ    ‚úì customer_name NOT EMPTY         ‚îÇ
‚îÇ    ‚úì restaurant_name NOT EMPTY       ‚îÇ
‚îÇ    ‚úì item NOT EMPTY                  ‚îÇ
‚îÇ    ‚úì order_status NOT EMPTY          ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ 4. Tag rows: SUCCESS / FAILED        ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ 5. Filter SUCCESS rows               ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ 6. Add date partition:               ‚îÇ
‚îÇ    date = YYYY-MM-DD                 ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ 7. Write to Parquet:                 ‚îÇ
‚îÇ    /datalake/.../output/orders/      ‚îÇ
‚îÇ    date=2025-11-18/part-*.parquet    ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ 8. Checkpoint offsets                ‚îÇ
‚îÇ    (for recovery)                    ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ 9. Trigger every 5 seconds           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Validation Rules**:
| Field | Rule | Action |
|-------|------|--------|
| `order_id` | NOT NULL | Drop if null |
| `amount` | > 0 | Drop if ‚â§ 0 |
| `customer_name` | NOT EMPTY | Drop if null/empty |
| `restaurant_name` | NOT EMPTY | Drop if null/empty |
| `item` | NOT EMPTY | Drop if null/empty |
| `order_status` | NOT EMPTY | Drop if null/empty |

#### 4. **Data Lake Storage**

**Output Structure**:
```
/datalake/food/2025em1100102/output/orders/
‚îú‚îÄ‚îÄ date=2025-11-18/
‚îÇ   ‚îú‚îÄ‚îÄ part-00000-abc123.parquet
‚îÇ   ‚îú‚îÄ‚îÄ part-00001-def456.parquet
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ date=2025-11-19/
‚îÇ   ‚îú‚îÄ‚îÄ part-00000-ghi789.parquet
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ ...
```

**Checkpoint Directory** (for recovery):
```
/datalake/food/2025em1100102/checkpoints/orders/
‚îú‚îÄ‚îÄ metadata
‚îú‚îÄ‚îÄ offsets
‚îî‚îÄ‚îÄ ...
```

---

## üíæ Database Schema

### PostgreSQL Table: `2025em1100102_orders`

```sql
CREATE TABLE "2025em1100102_orders" (
    order_id SERIAL PRIMARY KEY,
    customer_name VARCHAR(255) NOT NULL,
    restaurant_name VARCHAR(255) NOT NULL,
    item VARCHAR(255) NOT NULL,
    amount NUMERIC(10, 2) NOT NULL,
    order_status VARCHAR(50) CHECK (order_status IN ('PLACED', 'PREPARING', 'DELIVERED', 'CANCELLED')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Field Descriptions

| Field | Type | Constraints | Description |
|-------|------|-------------|-------------|
| `order_id` | SERIAL | PRIMARY KEY | Unique order identifier |
| `customer_name` | VARCHAR(255) | NOT NULL | Name of customer placing order |
| `restaurant_name` | VARCHAR(255) | NOT NULL | Name of restaurant |
| `item` | VARCHAR(255) | NOT NULL | Food item ordered |
| `amount` | NUMERIC(10,2) | NOT NULL | Order amount in currency |
| `order_status` | VARCHAR(50) | CHECK constraint | Status: PLACED, PREPARING, DELIVERED, CANCELLED |
| `created_at` | TIMESTAMP | DEFAULT NOW() | Record creation timestamp |

### Sample Data

10 sample orders are automatically inserted during PostgreSQL initialization:

```
Order #1: Alice Smith | Spice Garden | Butter Chicken | $350.00 | DELIVERED
Order #2: Bob Jones | Burger King | Whopper Meal | $250.50 | DELIVERED
Order #3: Charlie Brown | Pizza Hut | Margherita Pizza | $199.00 | DELIVERED
...
```

---

## üß™ Testing & Verification

### 1. Test PostgreSQL Connection

```bash
docker-compose exec spark-runner python3 scripts/test_postgres_connection.py
```

**Expected Output**:
```
‚úì Connected to PostgreSQL (postgres:5432)
‚úì Database: food_delivery_db
‚úì Table: 2025em1100102_orders
‚úì Record count: 10
```

### 2. Test Kafka Producer

```bash
docker-compose exec spark-runner python3 scripts/test_kafka_producer.py
```

**Expected Output**:
```
‚úì Connected to Kafka (kafka:9095)
‚úì Published test message to topic: 2025em1100102_food_orders_raw
‚úì Message ID: 123
```

### 3. Test Kafka Consumer

```bash
docker-compose exec spark-runner python3 scripts/test_kafka_consumer.py
```

**Expected Output**:
```
‚úì Connected to Kafka (kafka:9095)
‚úì Subscribed to topic: 2025em1100102_food_orders_raw
‚úì Received message: {"order_id": 1, ...}
```

### 4. Verify End-to-End Pipeline

```bash
docker-compose exec spark-runner python3 scripts/verify_pipeline.py --config configs/orders_stream.yml
```

**Expected Output**:
```
========== PostgreSQL Check ==========
‚úì Connected to PostgreSQL (postgres:5432)
‚úì Table '2025em1100102_orders' contains 10 records
‚úì Recent records:
    Order #10: Jack Black | $120.00 | 2025-11-18 12:00:00
    ...

========== Kafka Check ==========
‚úì Connected to Kafka (kafka:9095)
‚úì Topic '2025em1100102_food_orders_raw' exists
‚úì Partitions: 3
‚úì Messages in topic: 15

========== Data Lake Check ==========
‚úì Parquet files found: /datalake/food/2025em1100102/output/orders/
‚úì Date partitions: 2025-11-18, 2025-11-19
‚úì Total records in data lake: 15
‚úì No duplicates detected

========== Pipeline Status ==========
‚úì PIPELINE HEALTHY
```

### 5. Read Parquet Files

```bash
docker-compose exec spark-runner python3 scripts/read_parquet_records.py --config configs/orders_stream.yml
```

**Expected Output**:
```
Reading Parquet files from: /datalake/food/2025em1100102/output/orders/

Total records: 15
Sample records:
  Order #1: Alice Smith | Spice Garden | $350.00 | 2025-11-18
  Order #2: Bob Jones | Burger King | $250.50 | 2025-11-18
  ...
```

---

## üîç Monitoring & Debugging

### View Spark Master UI

Open browser: `http://localhost:9090`

Shows:
- Running applications
- Worker status
- Job execution details
- Stage information

### View Spark Worker UIs

- Worker A: `http://localhost:9091`
- Worker B: `http://localhost:9785`

### Check Container Logs

```bash
# Producer logs
docker-compose logs -f spark-runner | grep PRODUCER

# Consumer logs
docker-compose logs -f spark-runner | grep CONSUMER

# PostgreSQL logs
docker-compose logs -f postgres

# Kafka logs
docker-compose logs -f kafka
```

### Monitor Data Lake Growth

```bash
# Check output directory
docker-compose exec spark-runner ls -lh datalake/food/2025em1100102/output/orders/

# Count records in Parquet
docker-compose exec spark-runner python3 scripts/read_parquet_records.py --config configs/orders_stream.yml
```

### Check Last Processed Timestamp

```bash
docker-compose exec spark-runner cat datalake/food/2025em1100102/lastprocess/orders/last_processed_timestamp.txt
```

---

## üõë Stopping & Cleanup

### Stop All Services (Keep Data)

```bash
docker-compose stop
```

### Stop and Remove Containers (Keep Volumes)

```bash
docker-compose down
```

### Complete Cleanup (Remove Everything)

```bash
docker-compose down -v
```

‚ö†Ô∏è **Warning**: This removes all data volumes including PostgreSQL data and data lake files.

---

## üêõ Troubleshooting

### Issue: Producer Not Publishing Messages

**Symptoms**: No messages in Kafka topic

**Solutions**:
1. Check PostgreSQL has data: `docker-compose exec postgres psql -U student -d food_delivery_db -c "SELECT COUNT(*) FROM \"2025em1100102_orders\";"`
2. Check Kafka connectivity: `docker-compose exec spark-runner python3 scripts/test_kafka_producer.py`
3. Check producer logs: `docker-compose logs spark-runner | grep PRODUCER`
4. Verify config file: `cat configs/orders_stream.yml`

### Issue: Consumer Not Writing Parquet Files

**Symptoms**: Data lake directory is empty

**Solutions**:
1. Check Kafka has messages: `docker-compose exec spark-runner python3 scripts/test_kafka_consumer.py`
2. Check consumer logs: `docker-compose logs spark-runner | grep CONSUMER`
3. Verify checkpoint directory exists: `docker-compose exec spark-runner ls -la datalake/food/2025em1100102/checkpoints/`
4. Check Spark worker status: Visit `http://localhost:9091`

### Issue: PostgreSQL Connection Refused

**Symptoms**: `psycopg2.OperationalError: could not connect to server`

**Solutions**:
1. Check PostgreSQL is running: `docker-compose ps postgres`
2. Verify credentials in config: `postgres.user`, `postgres.password`
3. Check port mapping: `docker-compose port postgres 5432`
4. Restart PostgreSQL: `docker-compose restart postgres`

### Issue: Kafka Topic Not Found

**Symptoms**: `Topic does not exist`

**Solutions**:
1. Check topic exists: `docker-compose exec kafka kafka-topics --list --bootstrap-server kafka:9095`
2. Create topic manually: `docker-compose exec kafka kafka-topics --create --topic 2025em1100102_food_orders_raw --partitions 3 --replication-factor 1 --bootstrap-server kafka:9095`
3. Verify docker-compose.yml has topic creation config

### Issue: Out of Memory Errors

**Symptoms**: `java.lang.OutOfMemoryError`

**Solutions**:
1. Increase Spark memory in docker-compose.yml:
   ```yaml
   environment:
     - SPARK_DRIVER_MEMORY=4g
     - SPARK_EXECUTOR_MEMORY=4g
   ```
2. Reduce batch size: `consumer.maxOffsetsPerTrigger: 500`
3. Increase poll interval: `cdc.poll_interval_sec: 10`

---

## üìö Code Documentation

### Producer: `orders_cdc_producer.py`

**Main Functions**:

| Function | Purpose |
|----------|---------|
| `load_config()` | Load YAML configuration |
| `init_spark_session()` | Initialize Spark with Kafka connector |
| `read_last_processed_timestamp()` | Read state file for incremental processing |
| `write_last_processed_timestamp()` | Persist state after successful publish |
| `build_jdbc_url()` | Build PostgreSQL JDBC connection string |
| `build_cdc_query()` | Build SQL query for new records |
| `convert_dataframe_to_json()` | Convert DataFrame rows to JSON |
| `publish_to_kafka()` | Write JSON to Kafka topic |
| `run_cdc_polling_loop()` | Main polling loop (runs indefinitely) |

**Key Algorithm**:
```python
while True:
    last_ts, last_id = read_last_processed_timestamp()
    df = spark.read.jdbc(
        url, 
        query=f"SELECT * FROM orders WHERE created_at > '{last_ts}' OR (created_at = '{last_ts}' AND order_id > {last_id})"
    )
    if df.count() > 0:
        df_json = convert_dataframe_to_json(df)
        publish_to_kafka(df_json)
        max_ts = df.agg(max("created_at")).collect()[0][0]
        write_last_processed_timestamp(max_ts)
    sleep(poll_interval)
```

### Consumer: `orders_stream_consumer.py`

**Main Functions**:

| Function | Purpose |
|----------|---------|
| `load_config()` | Load YAML configuration |
| `init_spark_session()` | Initialize Spark with Kafka connector |
| `build_schema()` | Define DataFrame schema for orders |
| `build_kafka_consumer_config()` | Build Kafka consumer options |
| `apply_data_validation()` | Tag rows as SUCCESS/FAILED |
| `add_date_partition()` | Extract date from timestamp |
| `setup_streaming_writer()` | Configure Parquet writer |
| `run_streaming_consumer()` | Main streaming query (runs indefinitely) |

**Key Algorithm**:
```python
df_kafka = spark.readStream.format("kafka").options(...).load()
df_json = df_kafka.select(from_json(col("value"), schema).alias("data")).select("data.*")
df_validated = apply_data_validation(df_json)
df_success = df_validated.filter(col("validation_status") == "SUCCESS")
df_partitioned = add_date_partition(df_success)
query = df_partitioned.writeStream.format("parquet").partitionBy("date").start()
query.awaitTermination()
```

---

## üìñ Additional Resources

### Configuration Reference
See `configs/orders_stream.yml` for all available parameters and their descriptions.

### Docker Compose Reference
See `docker-compose.yml` for service definitions, port mappings, and volume configurations.

### Database Schema
See `db/orders.sql` for PostgreSQL table definition and sample data.

### Spark Documentation
- [Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Spark SQL Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
- [Spark JDBC Data Source](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)

### Kafka Documentation
- [Kafka Producer API](https://kafka.apache.org/documentation/#producerconfigs)
- [Kafka Consumer API](https://kafka.apache.org/documentation/#consumerconfigs)
- [Kafka Topics](https://kafka.apache.org/documentation/#topicconfigs)

---

## üë• For New Developers

### Getting Started Checklist

- [ ] Clone/navigate to project directory
- [ ] Install Docker & Docker Compose
- [ ] Read this README completely
- [ ] Review `configs/orders_stream.yml` to understand configuration
- [ ] Review `db/orders.sql` to understand data schema
- [ ] Start services: `docker-compose up -d`
- [ ] Run verification: `docker-compose exec spark-runner python3 scripts/verify_pipeline.py --config configs/orders_stream.yml`
- [ ] Start producer: `docker-compose exec spark-runner bash scripts/producer_spark_submit.sh`
- [ ] Start consumer: `docker-compose exec spark-runner bash scripts/consumer_spark_submit.sh`
- [ ] Monitor Spark UI: Open `http://localhost:9090`
- [ ] Check data lake: `docker-compose exec spark-runner ls -la datalake/food/2025em1100102/output/orders/`

### Key Files to Understand

1. **Start here**: `configs/orders_stream.yml` - Understand all configuration parameters
2. **Then read**: `producers/orders_cdc_producer.py` - Understand CDC polling logic
3. **Then read**: `consumers/orders_stream_consumer.py` - Understand streaming processing
4. **Reference**: `db/orders.sql` - Understand data schema
5. **Reference**: `docker-compose.yml` - Understand service setup

### Common Tasks

**Insert test data**:
```bash
docker-compose exec spark-runner python3 scripts/insert_test_orders.py --config configs/orders_stream.yml
```

**View recent orders in PostgreSQL**:
```bash
docker-compose exec postgres psql -U student -d food_delivery_db -c "SELECT * FROM \"2025em1100102_orders\" ORDER BY created_at DESC LIMIT 5;"
```

**View Parquet files**:
```bash
docker-compose exec spark-runner python3 scripts/read_parquet_records.py --config configs/orders_stream.yml
```

**Check pipeline health**:
```bash
docker-compose exec spark-runner python3 scripts/verify_pipeline.py --config configs/orders_stream.yml
```

---

## üìù License & Notes

This is a graded assignment for the Data Stores & Pipelines course.

**Student ID**: 2025em1100102

**Assignment**: Real-Time Food Delivery Streaming Pipeline

---

**Last Updated**: December 31, 2025
